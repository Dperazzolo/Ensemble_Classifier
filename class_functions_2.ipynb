{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pressed-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A notebook with all the class and functions needed \n",
    "\n",
    "# general library osed for the fllowing functions and classes: \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn.preprocessing\n",
    "import sklearn as sk \n",
    "import scipy as sy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut, PredefinedSplit\n",
    "# import method to create the confusion matrix: \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nonconformist.nc import NcFactory\n",
    "from nonconformist.icp import IcpClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clinical-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------ ( GENERAL FUNCTIONS assignament 1 )!!!!! ------------------------\n",
    "\n",
    "# dof function: --> calculate degrees of freedom\n",
    "# ---------- PARAMETERS ---------------\n",
    "# p = percentage of probability, that can be one of those values: [90,95,97,99]\n",
    "# dof = the degrees of freedom (num_classes-1)\n",
    "# -------------------------------------\n",
    "\n",
    "def dof(p,dof):\n",
    "    file_df = pd.read_csv((\"./dof.csv\"),sep=';', names=['df',90,95,97,99,999])\n",
    "    thr = file_df[p].iloc[dof-1]\n",
    "    return thr \n",
    "\n",
    "\n",
    "# mapping function: --> map a value into the correct interval\n",
    "# ---------- PARAMETERS ---------------\n",
    "# val = value to be mappe\n",
    "# inter = list of all possible intervals\n",
    "# -------------------------------------\n",
    "def mapping(val,inter):\n",
    "    for ls in inter:\n",
    "        sx = ls[0]\n",
    "        dx = ls[1]\n",
    "        if (val >= sx and val <= dx): \n",
    "            return str(ls)\n",
    "    # case in which the value is lower than the minimun interval\n",
    "    if val < inter[0][0]:\n",
    "        return str(inter[0])\n",
    "    # case in which the value is higher than the maximum interval\n",
    "    if val > inter[len(inter)-1][1]:\n",
    "        return str(inter[len(inter)-1])\n",
    "    \n",
    "\n",
    "\n",
    "# define X2_merge function algorithm\n",
    "# ---------- PARAMETERS ---------------\n",
    "# dataset: dataframe of the dataset\n",
    "# labels: label class\n",
    "# continuity: array to identify the continuity features\n",
    "# sigLevel: 90%,95%,97%,99% --> level of significance for the threashold. \n",
    "# dof: degrees of freedom (number_of_possible_classes - 1 ) ???\n",
    "# globalDisc: is the dictionary in which you need to store possible intervals. \n",
    "# -------------------------------------\n",
    "\n",
    "def chi2_thr(dataset, labels, continuity, sigLevel, degF, globalDisc): \n",
    "    \n",
    "    print(continuity)\n",
    "    \n",
    "    # convert to dataframe: \n",
    "    df = pd.DataFrame(dataset)\n",
    "    \n",
    "    # take the threshold level for the X2 from the file\n",
    "    thr = dof(sigLevel,degF)\n",
    "    \n",
    "    #take names of columns\n",
    "    cols_name=(df.columns.values) # take all the names of attributes of your dataset. \n",
    "    \n",
    "    #add labels\n",
    "    #print(df.shape[1])\n",
    "    df[df.shape[1]-1] = labels\n",
    "    #print(df.shape[1])\n",
    "    #add to continuity the true and false for the labels: \n",
    "    continuity.append(False)\n",
    "    \n",
    "    print(continuity)\n",
    "    \n",
    "    for idx in range(len(cols_name)):\n",
    "    #for idx in range(len(continuity)-1): \n",
    "        \n",
    "        i = cols_name[idx]  # take index name\n",
    "        #print(\"*****\", i, \"*****\")\n",
    "        #check if the index correspond to a numerical attribute\n",
    "        if continuity[i] == True: \n",
    "            \n",
    "            # sort the entire dataframe \n",
    "            df = df.sort_values(i)\n",
    "            \n",
    "            # take label sorted\n",
    "            label_sort = df[df.shape[1]-1].to_numpy()\n",
    "            \n",
    "            \n",
    "            # take sort attribute \n",
    "            #arr_sort = sorted(pd.to_numeric((df[i]), downcast=\"float\")) # creation of an array of values\n",
    "            arr_sort = df[i].to_numpy()\n",
    "            # cast to float\n",
    "            arr_sort = arr_sort.astype(float)\n",
    "            \n",
    "            # remove the nan of that column and also in the labels in the same indexes\n",
    "            [att_noNA, label_noNA] = (arr_sort[~np.isnan(arr_sort)], label_sort[~np.isnan(arr_sort)])                  \n",
    "                              \n",
    "            \n",
    "            # Create contingency matrix and add the sum columns: \n",
    "            cont_matrix = pd.crosstab(att_noNA, label_noNA)\n",
    "            cont_matrix['sum'] = cont_matrix[0]+cont_matrix[1]\n",
    "            \n",
    "            # take each pair of rows of contingency matrix, until n-1, for eache numerical attribute: \n",
    "            intervals = [[cont_matrix.index[i], cont_matrix.index[i+1]] for i in range(len(cont_matrix)-1)]\n",
    "            \n",
    "            # STEPS OF THE PROCESS: \n",
    "            print('-----------', i, '------------')\n",
    "            \n",
    "            # take all the lengths that you need\n",
    "            l = len(cont_matrix) # length of conting martix  \n",
    "            inter_len=len(intervals) # length of all intervals\n",
    "            \n",
    "            # choose a minimum number of intervals, to compress your numerical attribute. \n",
    "            max_compression = 30 # each numerical feature, will be compress to 20 possibile intervals. \n",
    "                       \n",
    "            while(inter_len > max_compression):\n",
    "                \n",
    "                chi2 = [] # list of chi2 \n",
    "                for k in range(l-1):\n",
    "\n",
    "                    #calculate expected and observed, for each class of labels: \n",
    "                    b = cont_matrix.iloc[[k,k+1]] # conting matrix for the pair\n",
    "                    #b = pd.crosstab(b,label_noNA)\n",
    "\n",
    "                    tot_1= b[0].iloc[0]+b[0].iloc[1] # tot over the first col \n",
    "                    tot_2= b[1].iloc[0]+b[1].iloc[1] # tot over the second column\n",
    "                    tot_sum = b['sum'].iloc[0]+b['sum'].iloc[1]  # tot over the 'sum' column\n",
    "\n",
    "                    #first col elements \n",
    "                    obs_1 = b[0].iloc[0] \n",
    "                    obs_2 = b[0].iloc[1]\n",
    "\n",
    "                    #second col elements \n",
    "                    obs_3 = b[1].iloc[0]\n",
    "                    obs_4 = b[1].iloc[1]\n",
    "\n",
    "\n",
    "                    # compute the expected and check if they are less than 0.5 or not. \n",
    "                    expected_1 = (b['sum'].iloc[0] * tot_1) / tot_sum # expected of the first element of the first row\n",
    "                    if expected_1 < 0.5: \n",
    "                        expected_1 = 0.5 \n",
    "                    expected_2 = (b['sum'].iloc[1] * tot_1) / tot_sum # expected of the second element of the first row\n",
    "                    if expected_2 < 0.5: \n",
    "                        expected_2 = 0.5 \n",
    "\n",
    "                    expected_3 = (b['sum'].iloc[0] * tot_2) / tot_sum # expected of the first element of the second row\n",
    "                    if expected_3 < 0.5: \n",
    "                        expected_3 = 0.5 \n",
    "                    expected_4 = (b['sum'].iloc[1] * tot_2) / tot_sum # expected of the second element of the second row\n",
    "                    if expected_4 < 0.5: \n",
    "                        expected_4 = 0.5 \n",
    "\n",
    "\n",
    "                    # compute the X2 \n",
    "                    x2 = (((obs_1-expected_1)**2)/expected_1 ) + (((obs_2-expected_2)**2)/expected_2) + (((obs_3-expected_3)**2)/expected_3) + (((obs_4-expected_4)**2)/expected_4)\n",
    "                    chi2.append(x2)\n",
    "                    # add to the dictionary of chi2 of this attribute\n",
    "                    # key of the dictionary is the index name of the numerical attribute \n",
    "                    # the value is the list of X2 for each pair of the clumns of that attribute\n",
    "                    # chi2[i] = chi2\n",
    "                   \n",
    "                    \n",
    "                # take the index of the minimum value chi2 on the list for each feature: \n",
    "                min_x2 = min(chi2)\n",
    "                idx_min = chi2.index(min_x2)\n",
    "                    \n",
    "                # check if the value of the chi2 is less(or equal) than the threshold. \n",
    "                # if it is less, the H0 is not supported, so merge the two rows, otherwise let it unchanged.  \n",
    "                if min_x2 <= thr: \n",
    "                    # remove the row after the index of the minimum: \n",
    "                    cont_matrix.drop(cont_matrix.index[idx_min+1], inplace=True)\n",
    "                #recompute the len of intervals\n",
    "                l = len(cont_matrix)\n",
    "                \n",
    "                #max_merge = max_merge - 1# update the merge total counter\n",
    "                        # compute the last intervals : \n",
    "                intervals = [[cont_matrix.index[i], cont_matrix.index[i+1]] for i in range(len(cont_matrix)-1)]\n",
    "                inter_len=len(intervals)\n",
    "                # DEBUGGING \n",
    "                #print(inter_len)\n",
    " \n",
    "            globalDisc[i] = intervals\n",
    "\n",
    "\n",
    "# create object of feature transformer\n",
    "# ---------- PARAMETERS ---------------\n",
    "# Different parameters for each method (__init__, fit, transform)\n",
    "# -------------------------------------\n",
    "\n",
    "class FeatureTransformer: \n",
    "    # significance Level and degrees of freedom to default are 90 and 1. \n",
    "    def __init__(self, featureBag, discretization, missing_categorical, missing_numerical,sigLevel=90,degF=1): \n",
    "        self.featureBag = featureBag\n",
    "        self.discretization = discretization\n",
    "        self.missing_categorical = missing_categorical\n",
    "        self.missing_numerical = missing_numerical\n",
    "        \n",
    "        # create and save the OHE (one hote encoder)\n",
    "        self.OneHotEncoder = sk.preprocessing.OneHotEncoder(sparse=False)\n",
    "\n",
    "        # all returning elements: \n",
    "        self.global_discret = {} # empty dictionary, to fill with chi2merge, for global discretizaion  \n",
    "        self.local_discret = {} # empty dictionary, for local discretizzation. \n",
    "        self.mean = {} # list for all the means \n",
    "        self.var = {} # list for all variance \n",
    "        self.ftBag = []\n",
    "        self.sigLevel = sigLevel # significance levele (90,95,97,99)\n",
    "        self.degF = degF # degrees of freedoms\n",
    "        \n",
    "        # elements for missing categorical: \n",
    "        self.missing_cat_drop = []\n",
    "        self.missing_cat_moda = {}\n",
    "        self.missing_cat_random_unique = {}\n",
    "        self.missing_cat_random_prob = {}\n",
    "        self.missing_cat_random_uniform_unique = {}\n",
    "        \n",
    "        # elements for missing numerical: \n",
    "        self.missing_num_avg = {}\n",
    "        self.missing_num_random_mean = {}\n",
    "        self.missing_num_random_std = {}\n",
    "        \n",
    "        # list of dropped rows for missing categorical\n",
    "        self.dropped_cat = []\n",
    "        \n",
    "        # list of dropped rows for missing numerical\n",
    "        self.dropped_num = []\n",
    "        \n",
    "        \n",
    "    def fit(self,x,y,continuity): \n",
    "        \n",
    "        # set continuity \n",
    "        self.continuity = continuity       \n",
    "        # remove all the nan\n",
    "        #[arr_noNAN, label_noNAN] = (arr_x[~np.isnan(arr_x)], label_bool[~np.isnan(arr_x)])\n",
    "        \n",
    "        # ------------------------------------- FEATURE BAG ------------------------------------------\n",
    "        \n",
    "        if self.featureBag == True: \n",
    "            bool_arr = [True,False] # if the array is an arry full only of true i take all the columns. \n",
    "            #sample_arr = list(np.random.choice(bool_arr, x.shape[1])) # choice bunch of random features. \n",
    "            \n",
    "            #for a better idea of bagging\n",
    "            sample_arr = [False for _ in range(x.shape[1])]\n",
    "            for _ in range(x.shape[1]): \n",
    "                sample_arr[random.randint(0,x.shape[1]-1)]=True\n",
    "            \"\"\"\n",
    "            v = np.where(sample_arr == True)\n",
    "            x = x[:, v] # take only columns with index equal to true \n",
    "            \"\"\"\n",
    "            self.ftBag = sample_arr\n",
    "            \n",
    "            #return self # if you run the self here all the following code is not correct because update only here. \n",
    "            v = [i for i, x in enumerate(self.ftBag) if x]\n",
    "            x = x[v] \n",
    "\n",
    "         \n",
    "        if self.featureBag == False:\n",
    "            # do anithing\n",
    "            self.ftBag = x\n",
    "            #return self\n",
    "               \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ------------------------------------- MISSING CATEGORICAL ------------------------------------------\n",
    "        if self.missing_categorical == 'drop':\n",
    "            # remove on the transform ( drop the rows containing nan values --> task computed by transformation )           \n",
    "            # save all rows to drop due to categorical rows with NaN\n",
    "            \n",
    "            \"\"\"\n",
    "            ls_na = []\n",
    "            \n",
    "            # --- do directly on the transform ( drop the rows containing nan values --> task computed by transformation ) ---\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == False:\n",
    "                    \n",
    "                    # take indexes of all nan values in current categorical column \n",
    "                    rows_with_nan = (x[pd.isna(x[i])]).index\n",
    "                    \n",
    "                    # convert indexes found to list\n",
    "                    index_rows_na = (rows_with_nan.values).tolist()\n",
    "                    \n",
    "                    # concat to the full list: \n",
    "                    ls_na = ls_na + index_rows_na\n",
    "            \n",
    "            # save the list of all rows:\n",
    "            self.missing_cat_drop = ls_na\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "        \n",
    "        if self.missing_categorical == 'most_frequent':\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == False: # check for columns that are categorical\n",
    "                    # take unique values and counting of each values\n",
    "                    mf, mf_count = np.unique((x[i]).dropna(), return_counts=True)\n",
    "                    # take the highe value: \n",
    "                    max_count = max(mf_count)\n",
    "                    # convert to list: \n",
    "                    mf_count = mf_count.tolist()\n",
    "                    # take the most frequent element by its index: \n",
    "                    moda = mf[mf_count.index(max_count)]\n",
    "                    self.missing_cat_moda[i] = moda\n",
    "           \n",
    "            \n",
    "        if self.missing_categorical == 'random':\n",
    "            # use \"np.random.coiche\" with parameters: size = 1, p = vector_prob\n",
    "            # vectori_prob is a vector containing the probability of each item inside the feature\n",
    "            # probability of each item is given by: count(item_occurrences)/len(feature).\n",
    "            # create lambda object to divide occurrences of each value of your categorical vector.\n",
    "            # in this way you can find the probability of each value. \n",
    "            \n",
    "            # create object/function to divide each element\n",
    "            # you need to round,the probability to the first decimal place, in way to obtain probability sum = 1 \n",
    "            # np.around(,decimals=1)\n",
    "            divide = (lambda v: ((v/l_s1))) \n",
    "            \n",
    "            \n",
    "        \n",
    "            # minus -1 on the loop because we removed the label column. \n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == False: # check for columns that are categorical\n",
    "                    # take the len of the feature vector: \n",
    "                    l_s1=len(x[i])\n",
    "                    \n",
    "                    # take each element of the coulmn uniquely(s1) removing NaN values, \n",
    "                    # and the number of occurrences of each element\n",
    "                    s1, s1_count = np.unique((x[i]).dropna(), return_counts=True)\n",
    "                    \n",
    "                    # divide each item by the total number of occurrences: \n",
    "                    vector_prob = divide((s1_count))\n",
    "                    \n",
    "                    # compute some check, to be correct: \n",
    "                    if (len(vector_prob) < len(s1)): \n",
    "                        #remove an element (the element with the smallest probability ) from bucket:\n",
    "                        idx = np.where(vector_prob == min(vector_prob))\n",
    "                        s1 = np.delete(s1, idx)\n",
    "                     \n",
    "                    if (len(vector_prob) > len(s1)):\n",
    "                        # remove an element from prob: \n",
    "                        m = min(vector_prob)\n",
    "                        vector_prob = np.delete(vector_prob, np.where(vector_prob == m))                  \n",
    "                    \n",
    "                    \n",
    "                    #Save all possible unique values and also vector of probabilities: \n",
    "                    self.missing_cat_random_unique[i] = s1\n",
    "                    # when you save the probability occurrence of each element, you need to round \n",
    "                    # the probability to the first decimal place, in way to obtain probability sum = 1 \n",
    "                    self.missing_cat_random_prob[i] = vector_prob\n",
    "\n",
    "                    \n",
    "            \n",
    "        if self.missing_categorical == 'random_uniform':\n",
    "            # random_uniform = each value has the same probability to came out. \n",
    "            # (for example: with 10 values, each value has 1/10 probabaility.)\n",
    "            \n",
    "            # Use np.random.choice() --> with parameters: array, size = 1 \n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == False:\n",
    "                    s1 = np.unique((x[i]).dropna())\n",
    "                    self.missing_cat_random_uniform_unique[i] = s1\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ------------------------------------- MISSING NUMERICAL --------------------------------------------\n",
    "        if self.missing_numerical == 'drop':\n",
    "            # same of missing categorical, but compute directly on the transform. \n",
    "            pass\n",
    "            \n",
    "            \n",
    "        if self.missing_numerical == 'avarage':\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == True:\n",
    "                    cl = x[i]\n",
    "                    tmp_me = np.mean(cl.dropna().to_numpy().astype(float)) \n",
    "                    self.missing_num_avg[i] = tmp_me\n",
    "            \n",
    "        if self.missing_numerical == 'random':\n",
    "            # use as function 'np.random.normal()--> loc = mean, scale = std, size = 1'\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == True:\n",
    "                    cl = x[i]\n",
    "                    tmp_me = np.mean(cl.dropna().to_numpy().astype(float)) \n",
    "                    tmp_std = np.std(cl.dropna().to_numpy().astype(float))\n",
    "                    self.missing_num_random_mean[i] = tmp_me\n",
    "                    self.missing_num_random_std[i] = tmp_std\n",
    "            \n",
    "        if self.missing_numerical == 'random_uniform':\n",
    "            # 'np.random.uniform() --> parameters: low = min, high = max, size = how_many_element'\n",
    "            pass\n",
    "       \n",
    "    \n",
    "        # ------------------------------------- DISCRETIZATION ------------------------------------------\n",
    "                                \n",
    "        if self.discretization == 'global': \n",
    "            # only numerical attributes (feature) must be converted into a set of attribute \n",
    "            # with chi2global function. \n",
    "            \n",
    "            # call the chi2merge algo: \n",
    "            chi2_thr(x, y, continuity, self.sigLevel, self.degF, self.global_discret)\n",
    "            \n",
    "            # mapping data from your numerical attribute, to your dictionary data    \n",
    "            \"\"\"\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "            \n",
    "                if continuity[i] == True:\n",
    "                    # take the set of inverlas for that numerical feature\n",
    "                    dic = self.global_discret.get(i)\n",
    "                    # map all the rows\n",
    "                    new_vals = []\n",
    "                    for j in range(df_na[i].shape[0]):\n",
    "                        # convert into float the current val\n",
    "                        v = float(df_na[i].iloc[j])\n",
    "                        # map the value to the correct interval\n",
    "                        r = mapping(v,dic)\n",
    "                        #append it to the list\n",
    "                        new_vals.append(r) \n",
    "\n",
    "                    # substitute vals with intervals:\n",
    "                    x[i] = new_vals        \n",
    "            \n",
    "            #x = x.iloc[:,:-1]\n",
    "            \n",
    "            #print(type(x[1]))      \n",
    "            # fit the one hot encoding with the coresponding dataset without labels:         \n",
    "            \"\"\"\n",
    "            \n",
    "        \n",
    "        if self.discretization == 'local': \n",
    "            # take the numerical attributes (feature). For each take unique values without duplicates. sort it.\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                \n",
    "                if continuity[i] == True:\n",
    "                    df_disc = x[i].replace('?', np.nan) \n",
    "                    df_i_disc_unic = (df_disc.unique()).astype(float)\n",
    "                    srt = np.sort(df_i_disc_unic) # srt is an np.array object. \n",
    "                    \n",
    "                    # add sorted list to the dictionary of local discretization: \n",
    "                    self.local_discret[i] = srt\n",
    "            \n",
    "        \n",
    "        if self.discretization == 'scaled': \n",
    "            # mean normalizzation (perform mean and standard deviation for each numerical attribute)            \n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if continuity[i] == True:\n",
    "                    # compute mean and standard deviation for each row\n",
    "                    row = x[i] # take the column  \n",
    "                    me = np.mean(row.dropna().to_numpy().astype(float))\n",
    "                    var = np.std(row.dropna().to_numpy().astype(float))\n",
    "                    #append all the mean and all the var\n",
    "                    self.mean[i]=me\n",
    "                    self.var[i]=var\n",
    "                    # append to the self variable\n",
    "                    \n",
    "                \n",
    "            \n",
    "        \n",
    "        if self.discretization == 'None': \n",
    "            x=x\n",
    "             \n",
    "\n",
    "       # After all preprocessing of fitting, compute fitting of you ine hot encoder \n",
    "        #self.OneHotEncoder.fit(x)          \n",
    "                \n",
    "        # return dataset and label:\n",
    "        # return arr_x, label_bool\n",
    "        \n",
    "        return self # to update the dates.\n",
    "\n",
    "        \n",
    "    def transform(self,x):   \n",
    "        \"\"\"\n",
    "            self.x = x # the dataframe \n",
    "            self.y = y # the label array\n",
    "\n",
    "            # remove the last column of \n",
    "            x = x.iloc[:, :-1]\n",
    "            # convert dataframe into numpy array. \n",
    "            arr_x = x.to_numpy()\n",
    "\n",
    "            le = sk.preprocessing.LabelEncoder() # encode labels and convert it into numpy array. \n",
    "            label_bool = le.fit_transform(y)# fit and convert automatically in numpy array.\n",
    "            \n",
    "        \"\"\" \n",
    "        \n",
    "         # ------------ Features Bag: ------------------------\n",
    "        \n",
    "        if self.featureBag == True: \n",
    "            #x = x.iloc[:, :-1]#remove label\n",
    "            #v = np.where(self.ftBag == True)\n",
    "            v = [i for i, x in enumerate(self.ftBag) if x]\n",
    "            x = x[v] \n",
    "                      \n",
    "        \n",
    "        elif self.featureBag == False: \n",
    "            x = x \n",
    "            \n",
    "\n",
    "        # ------------ Missing Categorical: ------------------------\n",
    "        \n",
    "        if self.missing_categorical == 'drop':\n",
    "            #print(\"ciao sono entrato\")\n",
    "            # simply drop and inplace, the list of categorical rows with NaN\n",
    "                        # do directly on the transform ( drop the rows containing nan values --> task computed by transformation )\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == False:\n",
    "                    \n",
    "                    # take indexes of all nan values in current categorical column \n",
    "                    rows_with_nan = (x[pd.isna(x[i])]).index\n",
    "                    \n",
    "                    # convert indexes found to list\n",
    "                    index_rows_na = (rows_with_nan.values).tolist()\n",
    "                    \n",
    "                    # append to list of dropped rows: \n",
    "                    self.dropped_cat.append(index_rows_na)\n",
    "                    \n",
    "                    x.drop(index_rows_na, inplace=True)\n",
    "                    \n",
    "            \n",
    "        if self.missing_categorical == 'most_frequent':\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == False:\n",
    "                    moda = self.missing_cat_moda.get(i)\n",
    "                    # replace NaN with moda \n",
    "                    x[i] = x[i].apply(lambda x: ( moda if x is np.nan else  x))\n",
    "            \n",
    "           \n",
    "            \n",
    "        if self.missing_categorical == 'random':\n",
    "            # use \"np.random.coiche\" with parameters: size = 1, p = vector_prob\n",
    "            # vectori_prob is a vector containing the probability of each item inside the feature\n",
    "            # probability of each item is given by: count(item_occurrences)/len(feature).\n",
    "            # create lambda object to divide occurrences of each value of your categorical vector.\n",
    "            # in this way you can find the probability of each value. \n",
    "            \n",
    "            def rnd_choice(i):\n",
    "                prob = FT.missing_cat_random_prob.get(i)\n",
    "                bucket = FT.missing_cat_random_unique.get(i)\n",
    "                # Normalizzation of probability vector. \n",
    "                prob /= prob.sum()\n",
    "                el_random = np.random.choice(bucket, size=1, p=prob)\n",
    "                element = el_random[0]\n",
    "                return element\n",
    "\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == False:\n",
    "                    # take randomly one oblect, with random choice, according to the distribution:                     \n",
    "                    # replace with random value \n",
    "                    x[i] = x[i].apply(lambda x: ( rnd_choice(i) if x is np.nan else  x))\n",
    "\n",
    "                    \n",
    "            \n",
    "        if self.missing_categorical == 'random_uniform':\n",
    "            # random_uniform = each value has the same probability to came out. \n",
    "            # (for example: with 10 values, each value has 1/10 probabaility.)\n",
    "            # Use np.random.choice() --> with parameters: array, size = 1 \n",
    "            \n",
    "            def rnd_uni_choice(i):\n",
    "                el = np.random.choice(self.missing_cat_random_uniform_unique.get(i),1)\n",
    "                return el[0]\n",
    "            \n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == False:\n",
    "                    x[i] = x[i].apply(lambda x: ( rnd_uni_choice(i) if x is np.nan else  x))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ------------ Missing Numerical: ------------------------\n",
    "        if self.missing_numerical == 'drop':\n",
    "            #print(\"ciao sono entrato\")\n",
    "            # I NEED TO DROP NUMERICAL HERE BECAUSE BEFORE I DROP SOME ROWS WIT CATEGORICAL NAN. \n",
    "            # TO AVOID INCONGRUENCES I NEED TO REMOVE NUMERICAL NAN AFTER I'VE REMOVED THE CATEGORICAL NAN'S.\n",
    "            \n",
    "            # remove on the transform ( drop the rows containing nan values --> task computed by transformation )           \n",
    "            # save all rows to drop due to NUMERICAL with NaN\n",
    "            ls_na = []\n",
    "            \n",
    "            # do directly on the transform ( drop the rows containing nan values --> task computed by transformation )\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == True:\n",
    "                    \n",
    "                    # take indexes of all nan values in current categorical column \n",
    "                    rows_with_nan = (x[pd.isna(x[i])]).index\n",
    "                    \n",
    "                    # convert indexes found to list\n",
    "                    index_rows_na = (rows_with_nan.values).tolist()\n",
    "                    \n",
    "                    # append dropped rows to list of numerical dropped rows\n",
    "                    self.dropped_num.append(index_rows_na)\n",
    "                    \n",
    "                    x.drop(index_rows_na, inplace=True)\n",
    "                    \n",
    "                    \n",
    "        if self.missing_numerical == 'avarage':\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == True:\n",
    "                    avg = self.missing_num_avg.get(i)\n",
    "                    # replace NaN with moda \n",
    "                    x[i] = x[i].apply(lambda x: ( avg if x is np.nan else  x))\n",
    "            \n",
    "            \n",
    "        if self.missing_numerical == 'random':\n",
    "            # use as function 'np.random.normal()--> loc = mean, scale = std, size = 1'\n",
    "            def rand_num(i): \n",
    "                me = self.missing_num_random_mean.get(i)\n",
    "                st = self.missing_num_random_std.get(i)\n",
    "                element = np.random.normal(me, st, 1)\n",
    "                return element\n",
    "            \n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == True:\n",
    "                    x[i] = x[i].apply(lambda x: ( rand_num(i) if x is np.nan else  x))\n",
    "            \n",
    "        if self.missing_numerical == 'random_uniform':\n",
    "            # 'np.random.uniform() --> parameters: low = min, high = max, size = how_many_element'\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "                if self.continuity[i] == True:\n",
    "                    l = x[i].astype(float).min()\n",
    "                    h = x[i].astype(float).max()\n",
    "                    x[i] = x[i].apply(lambda x: ( np.random.uniform(l,h,1) if x is np.nan else  x))\n",
    "            \n",
    "        # try to print dataset\n",
    "        #print(x.head)\n",
    "        #print(\"–-------------------------\")  \n",
    "        # check if there is nans \n",
    "        #print(x[x.isna().any(axis=1)])\n",
    "        \n",
    "        # return dataset and label:\n",
    "        # return arr_x, label_bool  \n",
    "        \n",
    "        # ------------ Discretizations: ------------------------ \n",
    "        \n",
    "        if self.discretization == 'global':\n",
    "            \n",
    "            # only numerical attributes (feature) must be converted into a set of attribute \n",
    "            # with chi2global function. \n",
    "            \n",
    "            # Compute fit and transform of one hot encoding: \n",
    "            # x = self.OneHotEncoder.fit_transform(x)\n",
    "            #x = self.OneHotEncoder.transform(x)\n",
    "            cols_name=(x.columns.values) # take all the names of attributes of your dataset. \n",
    "            for idx in range(len(cols_name)): \n",
    "                #take correct index \n",
    "                i = cols_name[idx]\n",
    "            \n",
    "                if self.continuity[i] == True:\n",
    "                    # take the set of inverlas for that numerical feature\n",
    "                    dic = self.global_discret.get(i)\n",
    "                    # map all the rows\n",
    "                    print(i)\n",
    "                    print(\"------------------------------\")\n",
    "                    new_vals = []\n",
    "                    for j in range(x[i].shape[0]):\n",
    "                        # convert into float the current val\n",
    "                        v = float(x[i].iloc[j])\n",
    "                        # map the value to the correct interval\n",
    "                        r = mapping(v,dic)\n",
    "                        #print(r)\n",
    "                        if r == None:\n",
    "                            print(\"----------------------------\")\n",
    "                            print(\"il valore che non riesco ad inserire è: \", v)\n",
    "                            print(\"gli intervalli possibili sono: \", dic)\n",
    "                            print(\"----------------------------\")\n",
    "                        #append it to the list\n",
    "                        new_vals.append(r) \n",
    "\n",
    "                    # substitute vals with intervals:\n",
    "                    x[i] = new_vals \n",
    "                   \n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "        if self.discretization == 'local': \n",
    "            # the numerical attributes (feature) take unique values without duplicates and sort it. \n",
    "            # ( you should transform into float )\n",
    "            \n",
    "            def check_local(arr,n): \n",
    "                vector = np.vectorize(np.float)\n",
    "                arr = vector(arr)\n",
    "                res = ((np.array(arr))<= float(n))\n",
    "                return res.astype(int)\n",
    "                                                   \n",
    "            \n",
    "            cols_name=(x.columns.values)#take all the cols names \n",
    "            for i in range(len(cols_name)): \n",
    "                #print(vals[i])\n",
    "                idx = cols_name[i] # index name\n",
    "                if self.continuity[idx] == True:\n",
    "                    # take the unique and sorted array: \n",
    "                    arr_sort = self.local_discret[idx]\n",
    "                    # apply digitize of numpy in order to binarize the vector: \n",
    "                    x[idx] = x[idx].apply(lambda el: check_local((arr_sort),el))\n",
    "            \n",
    "        \n",
    "        if self.discretization == 'scaled': \n",
    "            # substitute eache value of the numerical feature, with the normalized values. \n",
    "            # x' = x-median/var \n",
    "            \n",
    "            cols_name=(x.columns.values)#take all the cols names \n",
    "            for i in range(len(cols_name)): \n",
    "                #print(vals[i])\n",
    "                idx = cols_name[i] # index name\n",
    "                if self.continuity[idx] == True:\n",
    "                    me = self.mean.get(idx)\n",
    "                    var = self.var.get(idx)\n",
    "                    x[idx] = x[idx].apply(lambda x: ((float(x)-me)/var) if x!='nan' else x)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        if self.discretization == 'None': \n",
    "            x=x \n",
    "\n",
    "        \n",
    "            \n",
    "        return x # return transformed data     \n",
    "\n",
    "    \n",
    "# DEFINE A CLASS TO OHE THE RESULT OF YOUR FEATURE-TRANSFORMER:\n",
    "# ------------------------------\n",
    "# --- INIT PARAMETERS ---\n",
    "# typeDisc: type of discretization for the OHE \n",
    "# dataFit: dataset with which you should fit your OHE encoder\n",
    "# dataTr: datata that you want to OHE dased on type and dataFit\n",
    "# --- OUTPUTS ---> OHE of dataTR based on type of discrretization and fitted dataset.\n",
    "# ------------------------------\n",
    "class OHEencoding:\n",
    "    def __init__(self, typeDisc, dataFit, dataTr, continuity):\n",
    "        self.typeDisc = typeDisc\n",
    "        self.dataFit = dataFit\n",
    "        self.dataTr = dataTr\n",
    "        self.continuity = continuity\n",
    "        # create your OHE encoder\n",
    "        self.OneHotEncoder = sk.preprocessing.OneHotEncoder(sparse = False)\n",
    "        self.cols = []\n",
    "        \n",
    "\n",
    "    def fitTransform(self): \n",
    "        \n",
    "        if self.typeDisc == 'global':\n",
    "            self.OneHotEncoder.fit(self.dataFit.astype(str))\n",
    "            res_global = self.OneHotEncoder.transform(self.dataTr.astype(str))\n",
    "            \n",
    "            return res_global\n",
    "\n",
    "        if self.typeDisc == 'local': \n",
    "            res_local = [] # create an empty list, that will corresponds to the resulting row from the dataset: \n",
    "            cols_name=(self.dataFit.columns.values)#take all the cols names \n",
    "            for i in range(len(cols_name)): \n",
    "                #print(vals[i])\n",
    "                idx = cols_name[i] # index name\n",
    "                if self.continuity[idx] == False:\n",
    "                    self.cols.append(idx)\n",
    "                    ts = np.array(np.unique(self.dataFit[idx]))\n",
    "                    self.OneHotEncoder.fit(ts.reshape(-1,1))\n",
    "                    result_test = self.OneHotEncoder.transform(np.array(self.dataTr[idx]).reshape(-1,1)).tolist()\n",
    "                    #print(result_test[0])\n",
    "                    #print(type(result_test[0]))\n",
    "                    res_local.append(result_test[0])\n",
    "        \n",
    "                    #res[i]\n",
    "                if self.continuity[idx] == True:  \n",
    "                    tmp = self.dataTr[idx].tolist() \n",
    "                    tmp_list = tmp[0]#.tolist()\n",
    "                    res_local.append(tmp_list)\n",
    "                    #ts = np.array(self.dataFit[idx])\n",
    "                    #self.OneHotEncoder.fit(ts.reshape(-1,1))\n",
    "                    #tmp = self.OneHotEncoder.transform(np.array(self.dataTr[idx]).reshape(-1,1))\n",
    "                    #self.dataTr[idx] = list(tmp)\n",
    "            return res_local   \n",
    "        \n",
    "             \n",
    "            #res = self.dataTr[idx]\n",
    "            #self.OneHotEncoder.fit(self.dataFit[self.cols].astype(str))\n",
    "            #res = self.OneHotEncoder.transform(self.dataTr[self.cols].astype(str))                   \n",
    "        else:   \n",
    "            res_all = [] # create an empty list, that will corresponds to the resulting row from the dataset: \n",
    "            cols_name=(self.dataFit.columns.values)#take all the cols names \n",
    "            for i in range(len(cols_name)): \n",
    "                #print(vals[i])\n",
    "                idx = cols_name[i] # index name\n",
    "                if self.continuity[idx] == False:\n",
    "                    self.cols.append(idx)\n",
    "                    ts = np.array(np.unique(self.dataFit[idx]))\n",
    "                    self.OneHotEncoder.fit(ts.reshape(-1,1))\n",
    "                    result_test = self.OneHotEncoder.transform(np.array(self.dataTr[idx]).reshape(-1,1)).tolist()\n",
    "                    #print(result_test[0])\n",
    "                    #print(type(result_test[0]))\n",
    "                    res_all.append(result_test[0])\n",
    "\n",
    "\n",
    "                    #res[i]\n",
    "                if self.continuity[idx] == True:  \n",
    "                    tmp = self.dataTr[idx]\n",
    "                    #print(type(tmp[0]))\n",
    "                    #print(tmp[0])\n",
    "                    res_all.append(tmp[0])\n",
    "            \n",
    "            return res_all\n",
    "\n",
    "        \n",
    "    # create your OHE encorder: \n",
    "    # fit your OHE with dataFit\n",
    "    # convert you OHE encoder with dataTr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fiscal-spouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass selfOptimizingClassifier: \\n    # define the input statements of our object, to decide the optimizzation: \\n    def __init__(self, classifier, params_grid = None, k_opt = 5, calibration = 0.2, compute_proba = \\'model\\'): \\n        \\n        # assign \\n        self.classifier = classifier\\n        self.params_grid = params_grid\\n        self.params_win = None\\n        #check if kopt is a positive integer\\n        self.k_opt  = k_opt\\n        \\n        self.model = None\\n        self.conf_matrix = None # the object \"cm\" request by the assignament. \\n        self.calibrated_model = None \\n        self.calibration = calibration\\n        self.compute_proba = compute_proba\\n        \\n    #optimize: the best parameters in params grid for classifier are chosen via GridSearch according to \\n    #k opt using (Xt\\', Yt\\'), such best parameters are stored in params win dictionary.\\n    def __optimize(self,xt,yt):\\n        #find opt params win\\n        clf = None\\n        if self.k_opt == 0: #if k=0, simply apply a split in train and test\\n            ps = sk.model_selection.PredefinedSplit(np.random.choice[-1,0], size=len(yt))#bagging if k=0\\n            ps.get_n_splits()\\n            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=ps, n_jobs=2)\\n        if self.k_opt == 1:\\n            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=LeaveOneOut(), n_jobs=2)\\n        if self.k_opt >=2:\\n            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=self.k_opt, n_jobs=2)\\n\\n        # compute fitting here with parameters: \\n        #print(type(xt), type(yt))\\n        #xt = np.array(xt) \\n        #yt = np.array(yt)\\n        #print(type(xt), type(yt))\\n\\n        clf.fit(xt,yt)\\n\\n        return clf.best_params_\\n\\n\\n    #if calibrate != 0 a conformal prediction model is build by calibrating model on (Xc, Yc),\\n    #such a model is stored in an attribute called calibrated model\\n\\n    # compute conformal prediction. \\n    def __calibrate(self,xc,yc):\\n        nc_dt = NcFactory.create_nc(self.model)\\n        icp = IcpClassifier(nc_dt)\\n        icp.calibrate(xc,yc)\\n        return icp\\n\\n    def predict(self,x):\\n        if (x is not None) and len(x) !=0:\\n            try:\\n                return self.model.predict(x)\\n            except:\\n                return np.nan\\n        return np.nan\\n\\n#The predict proba method will return the probability returned by the model if compute proba = \\'model\\'\\n#the array of probabilities given by the model is returned, otherwise it is returned the (normalized) column\\n#corresponding to the prediction of the confusion matrix computed at step (b).\\n    def predict_proba(self,x):\\n        if self.compute_proba != \\'model\\' and self.compute_proba !=0:\\n            if self.conf_matrix is not None:\\n                #probabilities returned according to confusion matrix computed\\n                try:\\n                    proba = [(self.conf_matrix[:,pred])/self.conf_matrix[:,pred].sum() for pred in self.model.predict(x)]\\n                    return np.array(proba)                \\n                except:\\n                    return np.nan\\n            return np.nan\\n        else:\\n            if self.compute_proba == \\'model\\':\\n                #i pass xt because it is: x-xc\\n                y_pred_model=self.model.predict(self.xt) ### !!!!!!!! this are the predictions, NO PROB ARRAY\\n                return np.array(y_pred_model)\\n\\n    # eps (epsilon) correspondo to the significative of conformal prediction\\n    def conformal_predict(self,x,eps=0.1):\\n        if self.calibration != 0 :\\n            try:\\n                return self.calibrated_model.predict(x,eps)\\n            except:\\n                return np.nan\\n        print(\"Not feasible calibration = 0\")\\n        return np.nan\\n\\n\\n    def accuracy(self):\\n        if self.compute_proba != 0 and self.compute_proba != \\'model\\':\\n            if self.conf_matrix is not None:\\n                return self.conf_matrix.diagonal().sum() / self.sum_matrix\\n        return np.nan\\n        \\n    # x = dataset, y = labels\\n    def fitData(self,x,y):\\n        xd = x\\n        yd = y\\n        xt,yt = None, None\\n        xc,yc = None, None\\n        xp,yp = None, None\\n        \\n        # Divide in xt and yt\\n        if self.calibration != 0: \\n            if self.calibration == \\'bagging\\': \\n                print(\"calibration, associated to bagging\")\\n                xt, xc, yt, yc = bag_split(xd, yd)\\n                \\n            elif self.calibration > 0 and self.calibration < 1: # it is a value between 0 and 1. \\n                xt, xc, yt, yc = train_test_split(xd, yd, test_size = self.calibration)\\n        \\n        # if you arrive here, calibration set is 0 and xt and yt remain equal. \\n\\n        # Divide in x\\'t and y\\'t\\n        # recompute splitting to create  x\\'t and y\\'t and also xp and yp (xp and yp are used to create confusion matrix)\\n        if self.compute_proba != \\'model\\':\\n            if self.compute_proba == \\'bagging\\':\\n                print(\"probability set extracted with bagging\")\\n                xt, xp, yt, yp = bag_split(xd, yd)\\n            else:\\n                xt, xp, yt, yp = train_test_split(xd, yd, test_size=self.compute_proba)\\n        \\'\\'\\'             \\n        # prepare input data\\n        def prepare_inputs(xt, X_test):\\n            oe = OrdinalEncoder()\\n            oe.fit(X_train)\\n            X_train_enc = oe.transform(X_train)\\n            X_test_enc = oe.transform(X_test)\\n        return X_train_enc, X_test_enc\\n\\n        # prepare target\\n        def prepare_targets(y_train, y_test):\\n            le = LabelEncoder()\\n            le.fit(y_train)\\n            y_train_enc = le.transform(y_train)\\n            y_test_enc = le.transform(y_test)\\n        return y_train_enc, y_test_enc\\n                \\n        \\n        X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\\n        # prepare output data\\n        y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\\n        \\'\\'\\'  \\n        \\n        #Retrieve the best parameters \\n        self.params_win = self.__optimize(xt,yt)# best parameter are obtain by call optimize method and assign it to self.params_win\\n        \\n        # set the classifier with winning parameters: \\n        self.classifier.set_params(**self.params_win)#with ** mean that params_win has a dictionary structure\\n        \\n        # classifier is fitted with params_win on (Xt,Yt) and saved on a object attribute model, \\n        # represent the model obatin from the classifier\\n        self.model = self.classifier.fit(xt,yt)\\n        \\n        # compute the confusion matrix\\n        if self.compute_proba != \\'model\\' and self.compute_proba != 0:\\n            # compute the prediction by the classifier. \\n            y_pred = self.model.predict(xp)\\n            # compute and save confusion matrix \\n            self.conf_matrix = confusion_matrix(yp, y_pred)\\n            # we need now to normilize our confuzion matrix. \\n            # ??? sum_matrix ??? \\n            # self.sumMatrix = self.conf_matrix.sum().sum()\\n        \\n        #if calibrate != 0 the method calibrate is called\\n        if  self.calibration != 0 :\\n            self.calibrated_model = self.__calibrate(xc,yc)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------- GENERAL FUNCTIONS FROM ASSIGNAMENT 2 --------------------\n",
    "# ----------------------------ATTRIBUTES TO NUMBERS----------------------------\n",
    "# Description = from a dataset of categorical attributes, convert into integer. \n",
    "# Input = dataset (all attributes/features must be categorical)\n",
    "# Output = dataset with all attributes rows converted into integer. \n",
    "# -----------------------------------------------------------------------------\n",
    "def attributeToNum(dataset):\n",
    "    # take all attributes of the dataset: \n",
    "    attributes = dataset.columns.values\n",
    "    # define the encoder: \n",
    "    label_enc = sk.preprocessing.LabelEncoder()\n",
    "    # loop over all the columns: \n",
    "    for i in attributes:\n",
    "        # take the column (attributes) casted to string\n",
    "        new_att = dataset[i].astype(str) \n",
    "        # apply the fit transform encoding\n",
    "        new_att_transformed = label_enc.fit_transform(new_att)\n",
    "        # substitute column of dataset with new column transformed: \n",
    "        dataset.loc[:,[i]]=[new_att_transformed]\n",
    "        \n",
    "\n",
    "    # retrun transformed dataset: \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- ATTRIBUTES TO NUMBERS OBJECT CLASS ----------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "# Obecject class to convert data into all integer number based on labelEncoder of feat. \n",
    "# - use method \"convert\", to convert a new instance\n",
    "class attToNumClass: \n",
    "    def __init__(self, dataset):\n",
    "        # Create labelEncoder to use \n",
    "        self.label_enc = sk.preprocessing.LabelEncoder() \n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def convert(self, new_instance): \n",
    "        # take the columns \n",
    "        attributes = new_instance.columns.values\n",
    "        # loop over columns:\n",
    "        for i in attributes:\n",
    "            # take column of new instance and column of dataset. \n",
    "            # for the new attribute: \n",
    "            att_new_inst = new_instance[i].astype(str)\n",
    "            # for the dataset: \n",
    "            att_data = self.dataset[i].astype(str)\n",
    "            # fit self.label_enc on the self.dataset\n",
    "            self.label_enc.fit(att_data)\n",
    "            # transform the new_instance column using fitted LabelEncoder\n",
    "            att_new_inst_transformed = self.label_enc.transform(att_new_inst)\n",
    "            \n",
    "            # substitute column of dataset with new column transformed: \n",
    "            new_instance[i] = att_new_inst_transformed\n",
    "        # return new converted dataset. \n",
    "        return new_instance\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# --- allignDrop FUNCTION ---\n",
    "# Description: in case you use Drop in some where on Feature transformer, allign labels length\n",
    "# INPUT: FT --> FeatureTransformer object, y --> original label list. \n",
    "# OUTPUT: --> new label list, without dropped instances\n",
    "# -----------------------------------------------------------------------------\n",
    "def allignDrop(FT,y):\n",
    "    first_list = FT.dropped_num\n",
    "    second_list = FT.dropped_cat\n",
    "\n",
    "    # flat the two list: \n",
    "    flat_first_list = [item for sublist in first_list for item in sublist]\n",
    "    flat_second_list = [item for sublist in second_list for item in sublist]\n",
    "\n",
    "    # concatenate the two flatten lists: \n",
    "    total_to_remove = flat_first_list + flat_second_list\n",
    "    \n",
    "    # remove from the labels list: \n",
    "    new_y = np.delete(y, total_to_remove)\n",
    "    \n",
    "    return new_y\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "# ---------------- BAG SPLIT GENERIC FUNCTION --------------------\n",
    "# generic function to split data in a bagging way. \n",
    "def bag_split(x,y): \n",
    "    n = len(y)\n",
    "    a= np.array(range(0,n))\n",
    "    choose = np.zeros(n, dtype=bool)\n",
    "    for _ in range(0,n):\n",
    "        choose[np.random.choice(a)] = True\n",
    "    return x[choose], x[~choose], y[choose], y[~choose]\n",
    "\n",
    "\n",
    "# ----------- SELF OPTIMIZING CLASSIFIER OBJECT ------------------\n",
    "class selfOptimizingClassifier: \n",
    "    # define the input statements of our object, to decide the optimizzation: \n",
    "    def __init__(self, classifier, params_grid = None, k_opt = 5, calibration = 0.2, compute_proba = 'model'): \n",
    "        \n",
    "        # assign \n",
    "        self.classifier = classifier\n",
    "        self.params_grid = params_grid\n",
    "        self.params_win = None\n",
    "        #check if kopt is a positive integer\n",
    "        self.k_opt  = k_opt\n",
    "        \n",
    "        self.model = None\n",
    "        self.conf_matrix = None # the object \"cm\" request by the assignament. \n",
    "        self.SumMatrix = None\n",
    "        self.acc_conf = None\n",
    "        self.calibrated_model = None \n",
    "        self.calibration = calibration\n",
    "        self.compute_proba = compute_proba\n",
    "        #in order to do compute_proba=model we need to store the whole dataset minus xc---> so xt\n",
    "        self.xt=None\n",
    "        \n",
    "        \n",
    "    #optimize: the best parameters in params grid for classifier are chosen via GridSearch according to \n",
    "    #k opt using (Xt', Yt'), such best parameters are stored in params win dictionary.\n",
    "    def __optimize(self,xt,yt):\n",
    "        #find opt params win\n",
    "        clf = None\n",
    "        if self.k_opt == 0: #if k=0, simply apply a split in train and test\n",
    "            ps = sk.model_selection.PredefinedSplit(np.random.choice[-1,0], size=len(yt))#bagging if k=0\n",
    "            ps.get_n_splits()\n",
    "            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=ps, n_jobs=2)\n",
    "        if self.k_opt == 1:\n",
    "            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=LeaveOneOut(), n_jobs=2)\n",
    "        if self.k_opt >=2:\n",
    "            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=self.k_opt, n_jobs=2)\n",
    "\n",
    "\n",
    "        clf.fit(xt,yt)\n",
    "\n",
    "        return clf.best_params_\n",
    "\n",
    "\n",
    "    #if calibrate != 0 a conformal prediction model is build by calibrating model on (Xc, Yc),\n",
    "    #such a model is stored in an attribute called calibrated model\n",
    "\n",
    "    # compute conformal prediction. \n",
    "    def __calibrate(self,xc,yc):\n",
    "        nc_dt = NcFactory.create_nc(self.model)\n",
    "        icp = IcpClassifier(nc_dt)\n",
    "        icp.calibrate(xc,yc)\n",
    "        return icp\n",
    "\n",
    "    def predict(self,x):\n",
    "        if (x is not None) and len(x) !=0:\n",
    "            try:\n",
    "                return self.model.predict(x)\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "#The predict proba method will return the probability returned by the model if compute proba = 'model'\n",
    "#the array of probabilities given by the model is returned, otherwise it is returned the (normalized) column\n",
    "#corresponding to the prediction of the confusion matrix computed at step (b).\n",
    "    def predict_proba(self,x):\n",
    "        if self.compute_proba != 'model' and self.compute_proba !=0:\n",
    "            if self.conf_matrix is not None:\n",
    "                #probabilities returned according to confusion matrix computed\n",
    "                try:\n",
    "                    proba = [(self.conf_matrix[:,int(pred)])/self.conf_matrix[:,int(pred)].sum() for pred in self.model.predict(x)]\n",
    "                    return np.array(proba)                \n",
    "                except:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        else:\n",
    "            if self.compute_proba == 'model':\n",
    "                arr_prob= self.model.predict_proba(self.xt)\n",
    "                return np.array(arr_prob)\n",
    "\n",
    "    # eps (epsilon) correspond to the significative of conformal prediction\n",
    "    def conformal_predict(self,x,eps=0.1):\n",
    "        if self.calibration != 0 :\n",
    "            try:\n",
    "                return self.calibrated_model.predict(x,eps)\n",
    "            except:\n",
    "                return np.nan\n",
    "        print(\"Not feasible calibration = 0\")\n",
    "        return np.nan\n",
    "\n",
    "    #perform accuracy given the confusion matrix\n",
    "    def accuracy(self):\n",
    "        if self.compute_proba != 0 and self.compute_proba != 'model':\n",
    "            if self.conf_matrix is not None:\n",
    "                return self.conf_matrix.diagonal().sum() / self.SumMatrix\n",
    "        return np.nan\n",
    "    \n",
    "        \n",
    "    # x = dataset, y = labels\n",
    "    def fitData(self,x,y):\n",
    "        xd = x\n",
    "        yd = y\n",
    "        xt,yt = None, None\n",
    "        xc,yc = None, None\n",
    "        xp,yp = None, None\n",
    "        \n",
    "        # Divide in xt and yt\n",
    "        if self.calibration != 0: \n",
    "            if self.calibration == 'bagging': \n",
    "                print(\"calibration, associated to bagging\")\n",
    "                xt, xc, yt, yc = bag_split(xd, yd)\n",
    "                \n",
    "            elif self.calibration > 0 and self.calibration < 1: # it is a value between 0 and 1. \n",
    "                xt, xc, yt, yc = train_test_split(xd, yd, test_size = self.calibration)\n",
    "        else:\n",
    "            # if you arrive here, calibration set is 0 and xt and yt remain equal. \n",
    "            if self.calibation==0:\n",
    "                xt=xd\n",
    "                yt=yd\n",
    "                \n",
    "        \n",
    "   \n",
    "\n",
    "        # Divide in x't and y't\n",
    "        # recompute splitting to create  x't and y't and also xp and yp (xp and yp are used to create confusion matrix)\n",
    "        if self.compute_proba != 'model':\n",
    "            if self.compute_proba == 'bagging':\n",
    "                print(\"probability set extracted with bagging\")\n",
    "                xt, xp, yt, yp = bag_split(xd, yd)\n",
    "            else:\n",
    "                xt, xp, yt, yp = train_test_split(xd, yd, test_size=self.compute_proba)\n",
    "                \n",
    "        #control check for assign the parameters of compute_proba\n",
    "        #in this case xt because it represent x-xc\n",
    "        if self.compute_proba == 'model' and self.calibration !=0:\n",
    "            self.xt=xt\n",
    "            \n",
    "\n",
    "        \n",
    "        #Retrieve the best parameters \n",
    "        self.params_win = self.__optimize(xt,yt)# best parameter are obtain by call optimize method and assign it to self.params_win\n",
    "        \n",
    "        # set the classifier with winning parameters: \n",
    "        self.classifier.set_params(**self.params_win)#with ** mean that params_win has a dictionary structure\n",
    "        \n",
    "        self.classifier.get_params(deep=True)\n",
    "        # classifier is fitted with params_win on (Xt,Yt) and saved on a object attribute model, \n",
    "        # represent the model obatin from the classifier\n",
    "        self.model = self.classifier.fit(xt,yt)\n",
    "        \n",
    "        # compute the confusion matrix\n",
    "        if self.compute_proba != 'model' and self.compute_proba != 0:\n",
    "            # compute the prediction by the classifier. \n",
    "            y_pred = self.model.predict(xp)\n",
    "            # compute and save confusion matrix \n",
    "            self.conf_matrix = confusion_matrix(yp, y_pred)\n",
    "            \n",
    "            # we need now to normilize our confuzion matrix. \n",
    "            # ??? sum_matrix ??? \n",
    "            self.SumMatrix = self.conf_matrix.sum()\n",
    "            self.acc_conf = self.accuracy()\n",
    "            \n",
    "        #if self.compute_proba=='model':\n",
    "         #   print(self.xt)\n",
    "          #  self.compute_proba(self.xt)\n",
    "    \n",
    "        #if calibrate != 0 the method calibrate is called\n",
    "        if  self.calibration != 0 :\n",
    "            self.calibrated_model = self.__calibrate(xc,yc)\n",
    "        \n",
    "        return self\n",
    "\"\"\"\n",
    "class selfOptimizingClassifier: \n",
    "    # define the input statements of our object, to decide the optimizzation: \n",
    "    def __init__(self, classifier, params_grid = None, k_opt = 5, calibration = 0.2, compute_proba = 'model'): \n",
    "        \n",
    "        # assign \n",
    "        self.classifier = classifier\n",
    "        self.params_grid = params_grid\n",
    "        self.params_win = None\n",
    "        #check if kopt is a positive integer\n",
    "        self.k_opt  = k_opt\n",
    "        \n",
    "        self.model = None\n",
    "        self.conf_matrix = None # the object \"cm\" request by the assignament. \n",
    "        self.calibrated_model = None \n",
    "        self.calibration = calibration\n",
    "        self.compute_proba = compute_proba\n",
    "        \n",
    "    #optimize: the best parameters in params grid for classifier are chosen via GridSearch according to \n",
    "    #k opt using (Xt', Yt'), such best parameters are stored in params win dictionary.\n",
    "    def __optimize(self,xt,yt):\n",
    "        #find opt params win\n",
    "        clf = None\n",
    "        if self.k_opt == 0: #if k=0, simply apply a split in train and test\n",
    "            ps = sk.model_selection.PredefinedSplit(np.random.choice[-1,0], size=len(yt))#bagging if k=0\n",
    "            ps.get_n_splits()\n",
    "            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=ps, n_jobs=2)\n",
    "        if self.k_opt == 1:\n",
    "            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=LeaveOneOut(), n_jobs=2)\n",
    "        if self.k_opt >=2:\n",
    "            clf = GridSearchCV( estimator = self.classifier, param_grid = self.params_grid, cv=self.k_opt, n_jobs=2)\n",
    "\n",
    "        # compute fitting here with parameters: \n",
    "        #print(type(xt), type(yt))\n",
    "        #xt = np.array(xt) \n",
    "        #yt = np.array(yt)\n",
    "        #print(type(xt), type(yt))\n",
    "\n",
    "        clf.fit(xt,yt)\n",
    "\n",
    "        return clf.best_params_\n",
    "\n",
    "\n",
    "    #if calibrate != 0 a conformal prediction model is build by calibrating model on (Xc, Yc),\n",
    "    #such a model is stored in an attribute called calibrated model\n",
    "\n",
    "    # compute conformal prediction. \n",
    "    def __calibrate(self,xc,yc):\n",
    "        nc_dt = NcFactory.create_nc(self.model)\n",
    "        icp = IcpClassifier(nc_dt)\n",
    "        icp.calibrate(xc,yc)\n",
    "        return icp\n",
    "\n",
    "    def predict(self,x):\n",
    "        if (x is not None) and len(x) !=0:\n",
    "            try:\n",
    "                return self.model.predict(x)\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "#The predict proba method will return the probability returned by the model if compute proba = 'model'\n",
    "#the array of probabilities given by the model is returned, otherwise it is returned the (normalized) column\n",
    "#corresponding to the prediction of the confusion matrix computed at step (b).\n",
    "    def predict_proba(self,x):\n",
    "        if self.compute_proba != 'model' and self.compute_proba !=0:\n",
    "            if self.conf_matrix is not None:\n",
    "                #probabilities returned according to confusion matrix computed\n",
    "                try:\n",
    "                    proba = [(self.conf_matrix[:,pred])/self.conf_matrix[:,pred].sum() for pred in self.model.predict(x)]\n",
    "                    return np.array(proba)                \n",
    "                except:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        else:\n",
    "            if self.compute_proba == 'model':\n",
    "                #i pass xt because it is: x-xc\n",
    "                y_pred_model=self.model.predict(self.xt) ### !!!!!!!! this are the predictions, NO PROB ARRAY\n",
    "                return np.array(y_pred_model)\n",
    "\n",
    "    # eps (epsilon) correspondo to the significative of conformal prediction\n",
    "    def conformal_predict(self,x,eps=0.1):\n",
    "        if self.calibration != 0 :\n",
    "            try:\n",
    "                return self.calibrated_model.predict(x,eps)\n",
    "            except:\n",
    "                return np.nan\n",
    "        print(\"Not feasible calibration = 0\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    def accuracy(self):\n",
    "        if self.compute_proba != 0 and self.compute_proba != 'model':\n",
    "            if self.conf_matrix is not None:\n",
    "                return self.conf_matrix.diagonal().sum() / self.sum_matrix\n",
    "        return np.nan\n",
    "        \n",
    "    # x = dataset, y = labels\n",
    "    def fitData(self,x,y):\n",
    "        xd = x\n",
    "        yd = y\n",
    "        xt,yt = None, None\n",
    "        xc,yc = None, None\n",
    "        xp,yp = None, None\n",
    "        \n",
    "        # Divide in xt and yt\n",
    "        if self.calibration != 0: \n",
    "            if self.calibration == 'bagging': \n",
    "                print(\"calibration, associated to bagging\")\n",
    "                xt, xc, yt, yc = bag_split(xd, yd)\n",
    "                \n",
    "            elif self.calibration > 0 and self.calibration < 1: # it is a value between 0 and 1. \n",
    "                xt, xc, yt, yc = train_test_split(xd, yd, test_size = self.calibration)\n",
    "        \n",
    "        # if you arrive here, calibration set is 0 and xt and yt remain equal. \n",
    "\n",
    "        # Divide in x't and y't\n",
    "        # recompute splitting to create  x't and y't and also xp and yp (xp and yp are used to create confusion matrix)\n",
    "        if self.compute_proba != 'model':\n",
    "            if self.compute_proba == 'bagging':\n",
    "                print(\"probability set extracted with bagging\")\n",
    "                xt, xp, yt, yp = bag_split(xd, yd)\n",
    "            else:\n",
    "                xt, xp, yt, yp = train_test_split(xd, yd, test_size=self.compute_proba)\n",
    "        '''             \n",
    "        # prepare input data\n",
    "        def prepare_inputs(xt, X_test):\n",
    "            oe = OrdinalEncoder()\n",
    "            oe.fit(X_train)\n",
    "            X_train_enc = oe.transform(X_train)\n",
    "            X_test_enc = oe.transform(X_test)\n",
    "        return X_train_enc, X_test_enc\n",
    "\n",
    "        # prepare target\n",
    "        def prepare_targets(y_train, y_test):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(y_train)\n",
    "            y_train_enc = le.transform(y_train)\n",
    "            y_test_enc = le.transform(y_test)\n",
    "        return y_train_enc, y_test_enc\n",
    "                \n",
    "        \n",
    "        X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "        # prepare output data\n",
    "        y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "        '''  \n",
    "        \n",
    "        #Retrieve the best parameters \n",
    "        self.params_win = self.__optimize(xt,yt)# best parameter are obtain by call optimize method and assign it to self.params_win\n",
    "        \n",
    "        # set the classifier with winning parameters: \n",
    "        self.classifier.set_params(**self.params_win)#with ** mean that params_win has a dictionary structure\n",
    "        \n",
    "        # classifier is fitted with params_win on (Xt,Yt) and saved on a object attribute model, \n",
    "        # represent the model obatin from the classifier\n",
    "        self.model = self.classifier.fit(xt,yt)\n",
    "        \n",
    "        # compute the confusion matrix\n",
    "        if self.compute_proba != 'model' and self.compute_proba != 0:\n",
    "            # compute the prediction by the classifier. \n",
    "            y_pred = self.model.predict(xp)\n",
    "            # compute and save confusion matrix \n",
    "            self.conf_matrix = confusion_matrix(yp, y_pred)\n",
    "            # we need now to normilize our confuzion matrix. \n",
    "            # ??? sum_matrix ??? \n",
    "            # self.sumMatrix = self.conf_matrix.sum().sum()\n",
    "        \n",
    "        #if calibrate != 0 the method calibrate is called\n",
    "        if  self.calibration != 0 :\n",
    "            self.calibrated_model = self.__calibrate(xc,yc)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
